#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep  4 10:48:08 2023

@author: rowlandlabbci
"""

import datetime
import json
import glob
import itertools
import math
import pathlib
import pyperclip
import random
import sys
import time
import warnings
import pickle
import csv
import os
import matplotlib.pyplot as plt
import numpy as np
import re

from mlxtend.classifier import EnsembleVoteClassifier
from sklearn import svm, linear_model, discriminant_analysis, ensemble, tree, naive_bayes, neighbors, model_selection, metrics
from enum import Enum, auto
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import export_graphviz
from sklearn.exceptions import FitFailedWarning
import xgboost as xgb

from eeg_utils import *
from env_handler import *
from model_training_functions import *


# Define input parameters
verbose = True
specified_electrodes = EEG_Channel_ipcon_list()
model_iterations = 10
val_size = 0.3
phases = ['hold', 'reach']
patient_dir = "/home/changal/Documents/bmi_allen_rishi/patient_features_ipcon"
model_input = "/home/changal/Documents/bmi_allen_rishi/HVR_classification_CS_Stim/disease_prediction"
model_output = "/home/changal/Documents/bmi_allen_rishi/HVR_prediction_CS_Stim"
feature_type = 'psdbin'
patients = ['HC_Sham', 'HC_Stim', 'CS_Sham', 'CS_Stim']
# %%

# Obtain Patients
patient_dict = retreive_patient_info()
all_patients = {"HEALTHY":
                patient_dict["HC_Stim"] + patient_dict["HC_Sham"],
                "STROKE": patient_dict["CS_Stim"] + patient_dict["CS_Sham"]}
stim_type_patients = {"STIM":
                      patient_dict["HC_Stim"] + patient_dict["CS_Stim"],
                      "SHAM": patient_dict["HC_Sham"] + patient_dict["CS_Sham"]}


os.makedirs(model_output, exist_ok=True)
specified_features = getattr(Feat_Method, feature_type)

# Ignore Warnings
warnings.filterwarnings('ignore', category=FitFailedWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Create log file
LOG_FILE_DIRECTORY = ""

# Obtain Patients

# Perform model training 10 times and collect results
# Features generated by output_features.py


all_models = []
scores = []
train_scores = []
kappa_scores = []

subject_list = patient_dict["CS_Stim"]

subjects = retreive_patient_info()
groups = [(key, value) for key, value in subjects.items() if key in patients]


for x in [1]:

    pts = subject_list
    for state in ["intra5", "intra15", "post"]:

        all_x, all_y = [], []
        if verbose:
            print(
                f"Using feature selection method: {specified_features} with {specified_electrodes}")
            #print(f"\ton patient {patient_name}")
            print("Using state: ", state)
        for patient_name in pts:  # Extract Features

            # Load meta data
            patient_feature_folder = os.path.join(patient_dir, patient_name)
            with Path(os.path.join(patient_feature_folder, 'file_metadata.json')).open('r') as fp:
                metadata = json.load(fp)

            sample_rate = metadata['eeg_sample_rate']
            spec_sample_rate = metadata['spec_sample_rate']
            num_seconds = metadata[' seconds']

            # Find disease label
            disease_label = [
                key for key, values in subjects.items() if patient_name in values][0]

            trials = sorted([key for key, value in metadata.items() if key.startswith(
                'TRIAL') and len(key) > 1])  # get in numeric order
            if state == 'intra5':
                trial = trials[1]  # 1st
            elif state == 'intra15':
                trial = trials[2]  # 2nd
            elif state == 'post':
                print(trials)
                trial = trials[3]  # 3rd, only care about post5
            else:
                print('Error with state:', state)
                exit(-1)

            if verbose:
                print('Trials used:', trial)

            wk_trial_folder = os.path.join(
                patient_dir, patient_name, trial)

            for phase in phases:
                subject_features_csv = glob.glob(os.path.join(
                    wk_trial_folder, phase, '*' + specified_features.name+'.csv'))

                for electrode in specified_electrodes:
                    electrode_csv = [
                        x for x in subject_features_csv if electrode in x]
                    feature_val = []
                    for cs in electrode_csv:
                        feature_data = np.loadtxt(cs, delimiter=',')
                        if len(feature_data.shape) == 1:
                            feature_data = np.append(
                                feature_data, specified_electrodes.index(electrode))
                            feature_data = np.append(
                                feature_data, getattr(Feat_Method, phase).value)
                            feature_val.append(feature_data)
                        elif len(feature_data.shape) == 2:
                            electrode_data = [
                                specified_electrodes.index(electrode)] * len(feature_data)
                            phase_data = [
                                getattr(Feat_Method, phase).value] * len(feature_data)
                            feature_data = [np.concatenate((arr, np.array([x, y])), axis=0) for arr, x, y in zip(
                                feature_data, electrode_data, phase_data)]
                            feature_val.extend(feature_data)

                    '''disease_label = [
                        key for key, values in all_patients.items() if patient_name in values][0]'''

                    # phase labels
                    phase_label = 0 if phase == 'hold' else 1

                    feature_label = [phase_label] * len(feature_val)

                    all_x.extend(feature_val)
                    all_y.extend(feature_label)

        for i in range(0, len(all_y)):
            if all_y[i] == 1:
                all_y[i] = 0
            elif all_y[i] == 2:
                all_y[i] = 1

        print("all_y: ", all_y)
        scores = []
        kappa_scores = []

        # Make Results csv
        results = os.path.join(model_output + "_" + state)
        os.makedirs(results, exist_ok=True)
        fields = ["algorithm", "accuracy"]
        results_csv = Path(os.path.join(results, "results.csv"))
        with results_csv.open('a') as fp:
            csvwriter = csv.writer(fp)
            csvwriter.writerow(fields)

        fields = ["algorithm", "kappa"]
        results_csv_kappa = Path(os.path.join(results, "kappa.csv"))
        with (results_csv_kappa).open('a') as fp:
            csvwriter = csv.writer(fp)
            csvwriter.writerow(fields)

        # For each abstractable training algorithm
        for i in Class_Method:
            current_model = ""
            if i == Class_Method.LR:
                current_model = "LR"
            elif i == Class_Method.LDA:
                current_model = "LDA"
            elif i == Class_Method.DT:
                current_model = "DT"
            elif i == Class_Method.RF:
                current_model = "RF"
            elif i == Class_Method.NB:
                current_model = "NB"
            elif i == Class_Method.KNN:
                current_model = "KNN"
            elif i == Class_Method.ADA:
                current_model = "ADA"
            elif i == Class_Method.XG:
                current_model = "XG"
            train_stop = datetime.datetime.now()
            models_dir = os.path.join(results)
            for model_n in range(0, 10):
                # load model
                filename = os.path.join(
                    model_input, feature_type, current_model
                    + str(model_n) + ".sav")
                loaded_model = pickle.load(open(filename, 'rb'))
                score = loaded_model.score(all_x, all_y)
                print(current_model, ": ", score)
                scores.append(score)

                # kappa
                pred_y = loaded_model.predict(all_x)
                kappa = metrics.cohen_kappa_score(pred_y, all_y)
                kappa_scores.append(kappa)

                conf_matx = metrics.confusion_matrix(all_y, pred_y)
                val_stop = datetime.datetime.now()

                # Output confusion matrix
                with open(os.path.join(models_dir, f'model_conf_matrix_{current_model}.txt'), 'a') as fp:
                    fp.write(f"{i.name}\n")
                    for row in conf_matx:
                        fp.write(f"\t{row}\n")
                    fp.write("\n")

            with results_csv.open('a') as fp:
                csvwriter = csv.writer(fp)
                for x in scores:
                    row = [current_model, str(x)]
                    csvwriter.writerow(row)

            results_kappa_file_name = "_" + state + "_kappa.csv"
            with (Path(results) / results_kappa_file_name).open('a') as fp:
                csvwriter = csv.writer(fp)
                for x in kappa_scores:
                    row = [current_model, str(x)]
                    csvwriter.writerow(row)

        # score ensembles
        for ensemble in ["train", "global", "hard", "me", "uni"]:
            scores = []
            kappa_scores = []
            current_model = ensemble
            print(current_model)
            for model_n in range(0, 10):
                # load model
                filename = os.path.join(
                    model_input, feature_type, current_model +
                    str(model_n) + ".sav")
                loaded_model = pickle.load(open(filename, 'rb'))
                score = loaded_model.score(all_x, all_y)
                scores.append(score)

                # kappa
                pred_y = loaded_model.predict(all_x)
                kappa = metrics.cohen_kappa_score(pred_y, all_y)
                kappa_scores.append(kappa)

                # Output confusion matrix
                with open(os.path.join(models_dir, f'model_conf_matrix_{current_model}.txt'), 'a') as fp:
                    fp.write(f"{i.name}\n")
                    for row in conf_matx:
                        fp.write(f"\t{row}\n")
                    fp.write("\n")

            results = pathlib.Path(results)
            results_file_name = results_csv
            with (results / results_file_name).open('a') as fp:
                csvwriter = csv.writer(fp)
                for x in scores:
                    row = [current_model, str(x)]
                    csvwriter.writerow(row)

            results_kappa_file_name = "_" + state + "_kappa.csv"
            with (results / results_kappa_file_name).open('a') as fp:
                csvwriter = csv.writer(fp)
                for x in kappa_scores:
                    for model_n in range(0, 10):
                        # load model
                        filename = os.path.join(
                            model_input, feature_type, current_model + str(model_n) +
                            ".sav")
                        loaded_model = pickle.load(open(filename, 'rb'))
                        score = loaded_model.score(all_x, all_y)
                        scores.append(score)

                # kappa
                pred_y = loaded_model.predict(all_x)
                kappa = metrics.cohen_kappa_score(pred_y, all_y)
                kappa_scores.append(kappa)

                # Output confusion matrix
                with open(os.path.join(models_dir, f'model_conf_matrix_{current_model}.txt'), 'a') as fp:
                    fp.write(f"{i.name}\n")
                    for row in conf_matx:
                        fp.write(f"\t{row}\n")
                    fp.write("\n")
                    row = [current_model, str(x)]
                    csvwriter.writerow(row)
