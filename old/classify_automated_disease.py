import datetime
import json
import math
import pathlib
import random
import sys
import pickle
import os
import numpy as np
import warnings
import re

from eeg_utils import *
from env_handler import *
from model_training_functions import *
from patient_list import retreive_patient_info

from mlxtend.classifier import EnsembleVoteClassifier
from sklearn.exceptions import FitFailedWarning
from sklearn.model_selection import train_test_split


# Define input parameters
verbose = False
specified_electrodes = ['C3', 'C4', 'T3', 'Fp1', 'F7', 'T5', 'O1',
                        'F3', 'P3', 'A1', 'Fz', 'Cz', 'Fp2', 'F8', 'T4', 'T6',
                        'O2', 'F4', 'P4', 'A2', 'Fpz', 'Pz']
training_trial_idx = 1
model_iterations = 10
val_size = 0.3
phases = 'hold', 'prep', 'reach'
patient_dir = "/home/rowlandlabbci/Documents/Allen_Rishi_Paper/patient_features"
model_output = "/home/rowlandlabbci/Documents/Allen_Rishi_Paper/Model_classification"
feature_type = 'psdbin'
ip_con_switch = True  # WORK ON THIS LATER


# %% Perform model training 10 times and collect results
# Features generated by output_features.py

os.makedirs(model_output, exist_ok=True)
specified_features = getattr(Feat_Method, feature_type)


# Ignore Warnings
warnings.filterwarnings('ignore', category=FitFailedWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Create log file
LOG_FILE_DIRECTORY = ""

# Obtain Patients
patient_dict = retreive_patient_info()
all_patients = {"HEALTHY":
                patient_dict["HC_Stim"] + patient_dict["HC_Sham"],
                "STROKE": patient_dict["CS_Stim"] + patient_dict["CS_Sham"]}
subject_list = all_patients['HEALTHY'] + all_patients['STROKE']

# Perform model training 10 times and collect results
# Features generated by output_features.py

for train_trial in range(model_iterations):

    print(f"Iteration {train_trial + 1}")

    # Extract Features
    train_x, train_y, val_x, val_y = [], [], [], []
    for s_idx, subject in enumerate(subject_list):

        wk_trial_folder = sorted(glob.glob(os.path.join(
            patient_dir, subject, '*')))[training_trial_idx]

        feature_val = []
        for phase in phases:
            # find csv file for current phase
            subject_features_csv = glob.glob(os.path.join(
                wk_trial_folder, phase, '*' + specified_features.name+'*'))

            for electrode in specified_electrodes:
                # Find electrode csv
                electrode_csv = []
                for x in subject_features_csv:
                    match = re.search(
                        r'/\d_(.*?)_' + specified_features.name, x)
                    if match and match.group(1) == electrode:
                        electrode_csv.append(x)

                # Load in features
                for csv in electrode_csv:
                    # Collect binned PSD from each epoch
                    feature_data = np.loadtxt(csv, delimiter=',')

                    # Add phase and electrode as a feature
                    if len(feature_data.shape) == 1:
                        feature_data = np.append(
                            feature_data, getattr(EEG_Ch_val, electrode).value)
                        feature_data = np.append(
                            feature_data, getattr(Feat_Method, phase).value)
                        feature_data = np.append(
                            feature_data, getattr(Feat_Method, side).value)

                        feature_val.append(feature_data)

                    elif len(feature_data.shape) == 2:
                        electrode_data = [
                            getattr(EEG_Ch_val, electrode).value for x in feature_data]
                        phase_data = [
                            getattr(Feat_Method, phase).value for x in feature_data]
                        side_data = [
                            getattr(Feat_Method, side).value for x in feature_data]

                        feature_data = [np.concatenate((arr, np.array([x, y, z])), axis=0) for arr, x, y, z in zip(
                            feature_data, electrode_data, phase_data, side_data)]

                        feature_val.extend(feature_data)

             # Split data into training and testing
            disease_label = [key for key,
                             values in all_patients.items() if subject in values][0]
            feature_label = [
                getattr(Disease_State, disease_label).value for x in feature_val]
            sbj_x_train, sbj_x_val, sbj_y_train, sbj_y_val = train_test_split(
                feature_val, feature_label, test_size=val_size, random_state=42)

        train_x.extend(sbj_x_train)
        train_y.extend(sbj_y_train)
        val_x.extend(sbj_x_val)
        val_y.extend(sbj_y_val)

    all_models = []
    scores = []
    train_scores = []
    kappa_scores = []

    # For each abstractable training algorithm
    for i in Class_Method:
        current_model = ""
        train_start = datetime.datetime.now()
        # if i == Class_Method.SVM:
        #     current_model = 'SVM'
        #     model, params = train_svm(train_x, train_y)
        if i == Class_Method.LR:
            current_model = "LR"
            model, params = train_lr(train_x, train_y)
        elif i == Class_Method.LDA:
            current_model = "LDA"
            model, params = train_lda(train_x, train_y)
        elif i == Class_Method.DT:
            current_model = "DT"
            model, params = train_dt(train_x, train_y)
        elif i == Class_Method.RF:
            current_model = "RF"
            model, params = train_rf(train_x, train_y)
        elif i == Class_Method.NB:
            current_model = "NB"
            model, params = train_nb(train_x, train_y)
        elif i == Class_Method.KNN:
            current_model = "KNN"
            model, params = train_knn(train_x, train_y)
        elif i == Class_Method.ADA:
            continue  # fix
            current_model = "ADA"
            model, params = train_ada(train_x, train_y)
        elif i == Class_Method.XG:
            continue  # fix
            current_model = "XG"
            model, params = train_xg(train_x, train_y)
        train_stop = datetime.datetime.now()

        score = model.score(train_x, train_y)
        train_scores.append(score)
        score = model.score(val_x, val_y)

        # save model
        models_dir = os.path.join(
            model_output, "disease_prediction", feature_type)
        os.makedirs(models_dir, exist_ok=True)
        filename = os.path.join(
            models_dir, current_model + str(train_trial) + ".sav")
        pickle.dump(model, open(filename, 'wb'))

        # Output hyperparameters
        with open(os.path.join(models_dir, f'hyperparameters_{train_trial + 1}.txt'), 'a') as hp:
            model_string = "Model " + current_model + \
                " trial-" + str(train_trial + 1) + ": \n"
            hp.write(model_string)
            for p in params:
                hp.write("\t")
                param_string = p + ": " + str(params[p]) + "\n"
                hp.write(param_string)
            hp.write("\n")

        scores.append(score)
        pred_y = model.predict(val_x)
        kappa = metrics.cohen_kappa_score(pred_y, val_y)
        kappa_scores.append(kappa)

        # if i not in [Class_Method.ADA, Class_Method.XG, Class_Method.SVM]:
        if i not in [Class_Method.ADA, Class_Method.XG]:
            all_models.append(
                {'type': i, 'model': model, 'score': model.score(train_x, train_y)})

        val_start = datetime.datetime.now()
        val_res_y = model.predict(val_x)
        conf_matx = metrics.confusion_matrix(val_y, val_res_y)
        val_stop = datetime.datetime.now()

        # Output confusion matrix
        with open(os.path.join(models_dir, f'model_conf_matrix_{train_trial + 1}.txt'), 'a') as fp:
            fp.write(f"{i.name}\n")
            for row in conf_matx:
                fp.write(f"\t{row}\n")
            fp.write("\n")

        print(
            f"Model {i} scored \t{score*100:.2f}% in {train_stop - train_start} training time")
        if verbose:
            print("Confusion Matrix:")
            for row in conf_matx:
                print(f"\t{row}")
                print(f"Parameters: {json.dumps(params, indent=2)}")
                print(f"Training time:\t\t{train_stop - train_start}")
                print(f"Validation time:\t{val_stop - val_start}")
                print()
                print()

    if len(all_models) == 6:
        print(
            f"Voting Ensemble Order: {[x['type'].name for x in all_models]}")

        # Ensemble weighted by training accuracies
        train_model = EnsembleVoteClassifier([x['model'] for x in all_models], voting='soft', weights=[
            x['score'] for x in all_models], fit_base_estimators=False)
        train_model.fit(train_x, train_y)
        score = train_model.score(train_x, train_y)
        train_scores.append(score)

        val_start = datetime.datetime.now()
        score = train_model.score(val_x, val_y)
        scores.append(score)
        val_res_y = model.predict(val_x)
        conf_matx = metrics.confusion_matrix(val_y, val_res_y)
        kappa = metrics.cohen_kappa_score(val_y, val_res_y)
        kappa_scores.append(kappa)
        val_stop = datetime.datetime.now()

        with open(os.path.join(models_dir, f'model_conf_matrix_{train_trial + 1}.txt'), 'a') as fp:
            fp.write("train\n")
            for row in conf_matx:
                fp.write("\t".join(map(str, row)) + "\n")
            fp.write("\n")

        print(f"Model TRAIN scored \t\t{score * 100:.2f}%")
        if verbose:
            print(
                f"\t\tWeights used:\t{[x['score'] for x in all_models]}.")
            print(f"\t\tValidation time: {val_stop - val_start}")
        print()

        # Ensemble weighted by personal opinion
        # weights = [3, 1, 2, 1, 3, 2, 2]
        weights = [1, 2, 1, 3, 2, 2]
        me_model = EnsembleVoteClassifier(
            [x['model'] for x in all_models], voting='soft', weights=weights, fit_base_estimators=False)
        me_model.fit(train_x, train_y)
        score = me_model.score(train_x, train_y)
        train_scores.append(score)

        val_start = datetime.datetime.now()
        score = me_model.score(val_x, val_y)
        scores.append(score)
        val_res_y = model.predict(val_x)
        conf_matx = metrics.confusion_matrix(val_y, val_res_y)
        kappa = metrics.cohen_kappa_score(val_y, val_res_y)
        kappa_scores.append(kappa)
        val_stop = datetime.datetime.now()

        filename = models_dir + "me" + str(train_trial) + ".sav"
        pickle.dump(model, open(filename, 'wb'))

        with open(os.path.join(models_dir, f'model_conf_matrix_{train_trial + 1}.txt'), 'a') as fp:
            fp.write("me\n")
            for row in conf_matx:
                fp.write("\t".join(map(str, row)) + "\n")
            fp.write("\n")

        print(f"Model ME scored \t\t{score * 100:.2f}%")
        if verbose:
            print(f"\t\tWeights used:\t{weights}.")
            print(f"\t\tValidation time: {val_stop - val_start}")
        print()

        # Ensemble weighted by empirical global accuracies
        # weights = [.72662, .59793, .71133, .72925, .79051, .67535, .72198]
        weights = [.59793, .71133, .72925, .79051, .67535, .72198]
        global_model = EnsembleVoteClassifier(
            [x['model'] for x in all_models], voting='soft', weights=weights, fit_base_estimators=False)
        global_model.fit(train_x, train_y)
        score = global_model.score(train_x, train_y)
        train_scores.append(score)

        val_start = datetime.datetime.now()
        score = global_model.score(val_x, val_y)
        scores.append(score)
        val_res_y = model.predict(val_x)
        conf_matx = metrics.confusion_matrix(val_y, val_res_y)
        kappa = metrics.cohen_kappa_score(val_y, val_res_y)
        kappa_scores.append(kappa)
        val_stop = datetime.datetime.now()

        with open(os.path.join(models_dir, f'model_conf_matrix_{train_trial + 1}.txt'), 'a') as fp:
            fp.write(f"global\n")
            for row in conf_matx:
                fp.write(f"\t{row}\n")
                fp.write("\n")

        filename = models_dir + "global" + str(train_trial) + ".sav"
        pickle.dump(model, open(filename, 'wb'))

        print(f"Model GLOBAL scored \t\t{score * 100:.2f}%")
        if verbose:
            print(f"\t\tWeights used:\t{weights}.")
            print(f"\t\tValidation time: {val_stop - val_start}")
        print()

        # Ensemble with uniform weights
        uniform_model = EnsembleVoteClassifier(
            [x['model'] for x in all_models], voting='soft', fit_base_estimators=False)
        uniform_model.fit(train_x, train_y)
        score = uniform_model.score(train_x, train_y)
        train_scores.append(score)

        val_start = datetime.datetime.now()
        score = uniform_model.score(val_x, val_y)
        scores.append(score)
        val_res_y = model.predict(val_x)
        conf_matx = metrics.confusion_matrix(val_y, val_res_y)
        kappa = metrics.cohen_kappa_score(val_y, val_res_y)
        kappa_scores.append(kappa)
        val_stop = datetime.datetime.now()

        with open(os.path.join(models_dir, f'model_conf_matrix_{train_trial + 1}.txt'), 'a') as fp:
            fp.write("uniform\n")
            for row in conf_matx:
                fp.write("\t".join(map(str, row)) + "\n")
            fp.write("\n")

        filename = models_dir + "uni" + str(train_trial) + ".sav"
        pickle.dump(model, open(filename, 'wb'))

        print(f"Model UNI scored \t\t{score * 100:.2f}%")
        if verbose:
            print(f"\t\tValidation time: {val_stop - val_start}")
        print()

        # Ensemble with hard voting
        hard_model = EnsembleVoteClassifier(
            [x['model'] for x in all_models], voting='hard', fit_base_estimators=False)
        hard_model.fit(train_x, train_y)
        score = hard_model.score(train_x, train_y)
        train_scores.append(score)

        val_start = datetime.datetime.now()
        score = hard_model.score(val_x, val_y)
        scores.append(score)
        val_res_y = model.predict(val_x)
        conf_matx = metrics.confusion_matrix(val_y, val_res_y)
        kappa = metrics.cohen_kappa_score(val_y, val_res_y)
        kappa_scores.append(kappa)
        val_stop = datetime.datetime.now()

        filename = models_dir + "hard" + str(train_trial) + ".sav"
        pickle.dump(model, open(filename, 'wb'))

        with open(os.path.join(models_dir, f'model_conf_matrix_{train_trial + 1}.txt'), 'a') as fp:
            fp.write(f"hard\n")
            for row in conf_matx:
                fp.write("\t".join(map(str, row)) + "\n")
            fp.write("\n")

        print(f"Model HARD scored \t\t{score * 100: .2f}%")
        if verbose:
            print(f"\t\tValidation time: {val_stop - val_start}")
            print()

models_dir = pathlib.Path(models_dir)
with (models_dir / f'model_accuracies.csv').open('a') as fp:
    fp.write(','.join(str(x) for x in scores) + '\n')
with (models_dir / f'model_accuracies_train.csv').open('a') as fp:
    fp.write(','.join(str(x) for x in train_scores) + '\n')
with (models_dir / f'kappa.csv').open('a') as fp:
    fp.write(','.join(str(x) for x in kappa_scores) + '\n')
